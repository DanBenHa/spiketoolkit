{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON MODULE\n",
    "\n",
    "This notebook shows how to use the spiketoolkit.comparison module to:\n",
    "  1. compare pair of spike sorters\n",
    "  2. compare multiple spike sorters\n",
    "  3. extract units in agreement with multiple sorters (consensus-based)\n",
    "  4. run systematic performance comparisons on ground truth recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spiketoolkit as st\n",
    "import spikeextractors as se\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording, sorting_true = se.example_datasets.toy_example(duration=60, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Compare two spike sorters\n",
    "\n",
    "First, we will run two spike sorters and compare their ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_KL = st.sorters.run_klusta(recording)\n",
    "sorting_MS4 = st.sorters.run_mountainsort4(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compare_two_sorters` function allows us to compare the spike sorting output. It returns a `SortingComparison` object, with methods to inspect the comparison output easily. \n",
    "The comparison matches the units by comparing the agreement between unit spike trains.\n",
    "\n",
    "Let's see how to inspect and access this matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_KL_MS4 = st.comparison.compare_two_sorters(sorting1=sorting_KL, sorting2=sorting_MS4, \n",
    "                                               sorting1_name='klusta', sorting2_name='ms4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check which units were matched, the `get_mapped_sorting` methods can be used. If units are not matched they are listed as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units matched to klusta units\n",
    "mapped_sorting_klusta = cmp_KL_MS4.get_mapped_sorting1()\n",
    "print('Klusta units:', sorting_KL.get_unit_ids())\n",
    "print('Klusta mapped units:', mapped_sorting_klusta.get_mapped_unit_ids())\n",
    "\n",
    "# units matched to ms4 units\n",
    "mapped_sorting_ms4 = cmp_KL_MS4.get_mapped_sorting2()\n",
    "print('Mountainsort units:',sorting_MS4.get_unit_ids())\n",
    "print('Mountainsort mapped units:',mapped_sorting_ms4.get_mapped_unit_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_unit_spike_train` returns the mapped spike train. We can use it to check the spike times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that matched spike trains correspond\n",
    "_ = plt.plot(sorting_KL.get_unit_spike_train(7), \n",
    "         np.zeros(len(sorting_KL.get_unit_spike_train(7))), '|')\n",
    "_ = plt.plot(mapped_sorting_klusta.get_unit_spike_train(7),\n",
    "         np.ones(len(mapped_sorting_klusta.get_unit_spike_train(7))), '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compare multiple spike sorter outputs\n",
    "\n",
    "With 3 or more spike sorters, the comparison is implemented with a graph-based method. The multiple sorter comparison also allows to clean the output by applying a consensus-based method which only selects spike trains and spikes in agreement with multiple sorters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_TDC = st.sorters.run_tridesclous(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp = st.comparison.compare_multiple_sorters(sorting_list=[sorting_KL, sorting_MS4, sorting_TDC], \n",
    "                                              name_list=['KL', 'MS4', 'TDC'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple sorters comparison internally computes pairwise comparison, that can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['KL']['TDC'].get_mapped_sorting1().get_mapped_unit_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['KL']['MS4'].get_mapped_sorting1().get_mapped_unit_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp.sorting_comparisons['MS4']['TDC'].get_mapped_sorting1().get_mapped_unit_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a better agreement between tridesclous and mountainsort (5 units matched), while klusta only has two matched units with tridesclous, and three with mountainsort.\n",
    "\n",
    "\n",
    "\n",
    "## 3) Consensus-based method\n",
    "\n",
    "We can pull the units in agreement with different sorters using the `get_agreement_sorting` method. This allows to make spike sorting more robust by integrating the output of several algorithms. On the other hand, it might suffer from weak performance of single algorithms.\n",
    "\n",
    "When extracting the units in agreement, the spike trains are modified so that only the true positive spikes between the comparison with the best match are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_3 = mcmp.get_agreement_sorting(minimum_matching=3)\n",
    "print('Units in agreement for all three sorters: ', agr_3.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_2 = mcmp.get_agreement_sorting(minimum_matching=2)\n",
    "print('Units in agreement for at least sorters: ', agr_2.get_unit_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agr_all = mcmp.get_agreement_sorting()\n",
    "print('All units found: ', agr_all.get_unit_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit index of the different sorters can also be retrieved from the agreement sorting object (`agr_3`) property `sorter_unit_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agr_3.get_shared_unit_property_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agr_3.get_unit_property(9, 'sorter_unit_ids'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found our unit, we can plot a rasters with the spike trains of the single sorters and the one from the consensus based method. \n",
    "When extracting the agreement sorting, spike trains are cleaned so that only true positives remain from the comparison with the largest agreement are kept. \n",
    "Let's take a look at the raster plots for the different sorters and the agreement sorter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sorting_KL.get_unit_spike_train(7), \n",
    "         0*np.ones(len(sorting_KL.get_unit_spike_train(7))), '|')\n",
    "plt.plot(sorting_MS4.get_unit_spike_train(4), \n",
    "         1*np.ones(len(sorting_MS4.get_unit_spike_train(4))), '|')\n",
    "plt.plot(sorting_TDC.get_unit_spike_train(0), \n",
    "         2*np.ones(len(sorting_TDC.get_unit_spike_train(0))), '|')\n",
    "plt.plot(agr_3.get_unit_spike_train(9), \n",
    "         3*np.ones(len(agr_3.get_unit_spike_train(9))), '|')\n",
    "\n",
    "print('Klusta spike train length', len(sorting_KL.get_unit_spike_train(7)))\n",
    "print('Mountainsort spike train length', len(sorting_MS4.get_unit_spike_train(4)))\n",
    "print('Tridesclous spike train length', len(sorting_TDC.get_unit_spike_train(0)))\n",
    "print('Agreement spike train length', len(agr_3.get_unit_spike_train(9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best match is between Mountainsort and Tridesclous, but only the true positive spikes make up the agreement spike train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Compare spike sprting output with ground-truth recordings\n",
    "\n",
    "Simulated recordings or paired pipette and extracellular recordings can be used to validate spike sorting algorithms. \n",
    "\n",
    "For comparing to ground-truth data, the `compare_sorter_to_ground_truth(gt_sorting, tested_sorting)` function can be used.\n",
    "In this recording, we have ground-truth information for all units, so we can set `exhaustive_gt` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_MS4 = st.comparison.compare_sorter_to_ground_truth(sorting_true, sorting_MS4, exhaustive_gt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function first matches the ground-truth and spike sorted units, and then it computes several performance metrics.\n",
    "\n",
    "Once the spike trains are matched, each spike is labelled as:\n",
    "- true positive (tp): spike found both in `gt_sorting` and `tested_sorting`\n",
    "- false negative (fn): spike found in `gt_sorting`, but not in `tested_sorting`\n",
    "- false positive (fp): spike found in `tested_sorting`, but not in `gt_sorting`\n",
    "- misclassification errors (cl): spike found in `gt_sorting`, not in `tested_sorting`, found in another matched spike train of `tested_sorting`, and not labelled as true positives\n",
    "\n",
    "From the counts of these labels the following performance measures are computed:\n",
    "\n",
    "- accuracy: #tp / (#tp+ #fn + #fp)\n",
    "- recall: #tp / (#tp + #fn)\n",
    "- precision: #tp / (#tp + #fn)\n",
    "- miss rate: #fn / (#tp + #fn1) \n",
    "- false discovery rate: #fp / (#tp + #fp)\n",
    "\n",
    "The `get_performance` method a pandas dataframe (or a dictionary if `output='dict'`) with the comparison metrics. By default, these are calculated for each spike train of `sorting1`, the results can be pooles by average (average of the metrics) and by sum (all counts are summed and the metrics are computed then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_MS4.get_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the well and bad detected units. By default, the threshold on accuracy is 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_MS4.get_well_detected_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_MS4.get_false_positive_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_MS4.get_redundant_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_KL = st.comparison.compare_sorter_to_ground_truth(sorting_true, sorting_KL, exhaustive_gt=True)\n",
    "cmp_gt_KL.get_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_KL.get_well_detected_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_KL.get_false_positive_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_KL.get_redundant_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_gt_KL.get_bad_units()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Run systematic performance comparison\n",
    "\n",
    "This part of the notebook illustrates how to run systematic performance comparisons on ground truth recordings\n",
    "\n",
    "This will be done with mainly with 2 functions:\n",
    "  * **spiketoolkit.sorters.run_sorters** : this run several sorters on serevals dataset\n",
    "  * **spiketoolkit.comparison.gather_sorting_comparison** : this run several all possible comparison\n",
    "    with ground truth and results some metrics (accuracy, true positive rate, ..)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Generate several dataset with \"toy_example\"\n",
    "\n",
    "We first generate two recordings to be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec0, gt_sorting0 = se.example_datasets.toy_example(num_channels=4, duration=30, seed=10)\n",
    "rec1, gt_sorting1 = se.example_datasets.toy_example(num_channels=4, duration=30, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check which spike sorters are available, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sorters.available_sorters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run several sorters on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is really verbose due to some sorter so switch off output console\n",
    "\n",
    "recording_dict = {'toy_tetrode_1' : rec0, 'toy_tetrode_2': rec1}\n",
    "sorter_list = ['klusta', 'tridesclous', 'mountainsort4']\n",
    "path = Path('comparison_example')\n",
    "working_folder = path / 'working_folder'\n",
    "if working_folder.is_dir():\n",
    "    shutil.rmtree(str(working_folder))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "st.sorters.run_sorters(sorter_list, recording_dict, working_folder, engine=None)\n",
    "t1 = time.perf_counter()\n",
    "print('total run time', t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Collect dataframes for comparison\n",
    "\n",
    "As shown previously, the performance is returned as a pandas dataframe. The `gather_sorting_comparison` function, gathers all the outputs in the working folder and merges them in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = {'toy_tetrode_1': gt_sorting0, 'toy_tetrode_2': gt_sorting1}\n",
    "\n",
    "comp, dataframes = st.comparison.gather_sorting_comparison(working_folder, ground_truths, use_multi_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Display comparison tables\n",
    "\n",
    "Pandas dataframes can be nicely displayed as tables in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['perf_pooled_with_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['perf_pooled_with_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['run_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Easy plot with seaborn\n",
    "\n",
    "Seaborn allows to easily plot pandas dataframes. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = dataframes['run_times'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "sn.barplot(data=run_times, x='rec_name', y='run_time', hue='sorter_name', ax=ax)\n",
    "ax.set_title('Run times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = dataframes['perf_pooled_with_average'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "sn.barplot(data=perfs, x='rec_name', y='recall', hue='sorter_name', ax=ax)\n",
    "ax.set_title('Recall')\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = dataframes['perf_pooled_with_sum'].reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "ax = sn.barplot(data=perfs, x='rec_name', y='accuracy', hue='sorter_name', ax=ax)\n",
    "ax.set_title('accuracy')\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showed the capabilities of `spiketoolkit` to perform pair-wise comparisons between spike sorting outputs, comparisons among multiple sorters and consensus-based spike sorting, and systematic comparisons for grount-truth data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
